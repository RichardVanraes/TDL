{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 03 - Temporal Difference Learning - Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cliff walking\n",
    "\n",
    "Implement Q-learning and SARSA to solve the cliff walking environment. The cliff walking environment is part of the gymgrid package. \n",
    "To install gymgryd: **pip3 install gymgrid**. More information can be found here: https://pypi.org/project/gymgrid/.\n",
    "The goal for the agent is to find its way from start to finish without falling from the cliff. This is a standard episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is -1 on all transitions except those into the region marked Cliff. Stepping into this region incurs a reward of optimal path -100 and sends the agent instantly back to the start.\n",
    "\n",
    "- Implement the Q-learning algorithm. Write your own implementation and tune the hyperparameters (learning rate, discount factor, epsilon). Use epsilon decay if necessary. \n",
    "- Record the learning history. This means that for each episode you register the total episodic reward. After learning, plot the training history (x-axis = episode number; y-axis: total episodic reward). Especially in the beginning, the episodic rewards can be erratic because the agent is still exploring and has little knowledge of the environment. Therefore it can be a good idea to plot the moving average of the episodic rewards. For example, the average episodic reward achieved over 10 episodes.\n",
    "- Check if you can make the agent learn faster by using an adaptive learning rate. You start with a high learning rate and you decrease gradually as the agent learns.\n",
    "- Can you change the reward function in order to make the agent learn faster. For example by changing the reward values or by adding new rewards that make the reward function less sparse.\n",
    "\n",
    "- Modify you Q-learing implementation to a SARSA implementation.\n",
    "\n",
    "- Compare Q-learing with SARSA in terms of speed of learning, but especially in terms of the learned policy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to myself**: I have to use the conda gym environment for this notebook. I have to go to the conda powershell terminal and type: conda activate gym. Then I can use pip install gymgrid.\n",
    "*Use the conda gym environment for this notebook.*\n",
    "- go to conda powershell terminal\n",
    "- conda activate gym\n",
    "- pip install gymgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cliff walking\n",
    "\n",
    "import gymgrid #pip install gymgrid\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "class Qagent:\n",
    "    def __init__(self, nr_states, nr_actions, alpha, gamma, exploration_rate, decay):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.decay = decay\n",
    "        self.nr_states = nr_states\n",
    "        self.nr_actions = nr_actions\n",
    "        self.current_state = 0\n",
    "        self.previous_state = 0\n",
    "        self.possibleActions = [0, 1, 2, 3]\n",
    "        self.q_table = np.zeros((nr_states, nr_actions))\n",
    "        \n",
    "    def set_exploration_rate(self, exploration_rate):\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def compute_action(self, state): #state is integer in nr_states()\n",
    "        self.state = state\n",
    "        self.exploration_rate = self.exploration_rate*self.decay\n",
    "        self.r = random.random()\n",
    "        if self.r <= self.exploration_rate:\n",
    "            self.action = random.sample(self.possibleActions, 1)[0] # random.sample returns a list() of one item therefore we need to pick the first element [0] of that list\n",
    "            # print('exploration')\n",
    "        else:\n",
    "            self.action = np.argmax(self.q_table[self.state,:])\n",
    "            # print(self.r, self.exploration_rate, 'exploitation')\n",
    "        \n",
    "        return self.action\n",
    "    \n",
    "    def update_qtable(self, new_state, state, reward):\n",
    "        self.reward = reward\n",
    "        self.new_state = new_state\n",
    "        self.state = state\n",
    "        \n",
    "        self.r = random.random()\n",
    "        if self.r <= self.exploration_rate:\n",
    "            self.next_q_value = self.q_table[self.new_state, random.sample(self.possibleActions, 1)[0]]\n",
    "        else:\n",
    "            self.next_q_value = self.q_table[self.new_state, np.argmax(self.q_table[self.new_state, :])]\n",
    "        \n",
    "        self.q_table[self.state, self.action] = (1 - self.alpha)*self.q_table[self.state,self.action]+self.alpha*(self.reward+self.gamma*self.next_q_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States =  48\n",
      "Actions =  4\n"
     ]
    }
   ],
   "source": [
    "# load the environment\n",
    "env = gym.make('cliff-v0')\n",
    "\n",
    "# Define a walking function\n",
    "def cliffwalk(James):\n",
    "    new_state = env.reset()\n",
    "    done = False\n",
    "    t = 0\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        t += 1\n",
    "        current_state=new_state\n",
    "        env.render()\n",
    "        action = James.compute_action(state=current_state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        James.update_qtable(new_state,current_state,reward)\n",
    "        episode_reward = episode_reward + reward\n",
    "    timesteps = t\n",
    "    return episode_reward, timesteps\n",
    "\n",
    "def moving_average(a, n=2):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] -ret[:n]\n",
    "    return ret[n-1:]/n\n",
    "\n",
    "# Gymgrid properties\n",
    "env_states = env.observation_space.n # width = 12 x height = 4\n",
    "env_actions = env.action_space.n # gymgrid has 8 possible actions if you count the diagonal actions as well\n",
    "print(\"States = \",env_states)\n",
    "print(\"Actions = \", env_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliff opened\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (40,) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Richard\\Development\\AI@Home\\Reinforcement\\TDL\\TDL\\Session_03_Temporal_Diffence_Learning_Assignment.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     SAepisode_timesteps\u001b[39m.\u001b[39mappend(Stimestep)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m QLepisode_mean_rewards \u001b[39m=\u001b[39m moving_average(np\u001b[39m.\u001b[39;49marray(QLepisode_rewards),\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m SAepisode_mean_rewards \u001b[39m=\u001b[39m moving_average(np\u001b[39m.\u001b[39marray(SAepisode_rewards),\u001b[39m10\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinal Q-table with Q-learning\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Richard\\Development\\AI@Home\\Reinforcement\\TDL\\TDL\\Session_03_Temporal_Diffence_Learning_Assignment.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmoving_average\u001b[39m(a, n\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcumsum(a, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     ret[n:] \u001b[39m=\u001b[39m ret[n:] \u001b[39m-\u001b[39;49mret[:n]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Richard/Development/AI%40Home/Reinforcement/TDL/TDL/Session_03_Temporal_Diffence_Learning_Assignment.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ret[n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\u001b[39m/\u001b[39mn\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (40,) (10,) "
     ]
    }
   ],
   "source": [
    "# Qagent properties\n",
    "alpha = 0.8\n",
    "gamma = 0.8\n",
    "\n",
    "# Q-learning\n",
    "QLepsilon = 0 # Greedy + Q-learning\n",
    "QLdecay = 1 # Doesn't matter which value\n",
    "\n",
    "# SARSA\n",
    "Sepsilon = 1\n",
    "Sdecay = 0.95\n",
    "\n",
    "# Episode properties\n",
    "episodes = 50\n",
    "\n",
    "# Initialize collections\n",
    "QLepisode_rewards = []\n",
    "QLepisode_timesteps = []\n",
    "SAepisode_rewards = []\n",
    "SAepisode_timesteps = []\n",
    "\n",
    "# Start walking with two agents\n",
    "QL_James = Qagent(env_states, env_actions, alpha=alpha, gamma=gamma, exploration_rate=QLepsilon, decay = QLdecay)\n",
    "SA_James = Qagent(env_states, env_actions, alpha=alpha, gamma=gamma, exploration_rate=Sepsilon, decay = Sdecay)\n",
    "\n",
    "print(\"Cliff opened\")\n",
    "for i in range(episodes):\n",
    "    # Q-learning\n",
    "    Qreward, Qtimestep = cliffwalk(QL_James)\n",
    "    QLepisode_rewards.append(Qreward)\n",
    "    QLepisode_timesteps.append(Qtimestep)\n",
    "        \n",
    "    # SARSA\n",
    "    Sreward, Stimestep = cliffwalk(SA_James)\n",
    "    SAepisode_rewards.append(Sreward)\n",
    "    SAepisode_timesteps.append(Stimestep)\n",
    "env.close()\n",
    "       \n",
    "QLepisode_mean_rewards = moving_average(np.array(QLepisode_rewards),10)\n",
    "SAepisode_mean_rewards = moving_average(np.array(SAepisode_rewards),10)\n",
    "\n",
    "print(\"Final Q-table with Q-learning\")\n",
    "print(QL_James.q_table)\n",
    "\n",
    "print(\"Final Q-table with SARSA\")\n",
    "print(SA_James.q_table)\n",
    "\n",
    "   \n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.plot(QLepisode_rewards, label='Q-learning')\n",
    "ax1.plot(QLepisode_mean_rewards, label='Q-learning - mean')\n",
    "ax1.plot(SAepisode_rewards, label='SARSA')\n",
    "ax1.plot(SAepisode_mean_rewards, label='SARSA - mean')\n",
    "ax2.plot(QLepisode_timesteps, label='Q-learning')\n",
    "ax2.plot(SAepisode_timesteps, label='SARSA')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('Cliff closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The cartpole environment\n",
    "\n",
    "Solve the cartpole environment by means of both Q-learning and SARSA. \n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Q-learing and SARSA take discrete states as inputs. The cartpole environments outputs continuous state vectors. \n",
    "Therefore you will have to discretize these continuous state vectors. \n",
    "For example the cart position can vary in the range of -2.4 -> 2.4. You will have to discretize this range in a number of bins (for example 10 bins).\n",
    "The numpy function 'digitize' can be used for discretization. More information about this function: https://numpy.org/doc/stable/reference/generated/numpy.digitize.html\n",
    "\n",
    "- Implement the Q-learning algorithm. Write your own implementation and tune the hyperparameters (learning rate, discount factor, epsilon). Use epsilon decay if necessary. \n",
    "- Record the learning history. This means that for each episode you register the total episodic reward. After learning, plot the training history (x-axis = episode number; y-axis: total episodic reward). Especially in the beginning, the episodic rewards can be erratic because the agent is still exploring and has little knowledge of the environment. Therefore it can be a good idea to plot the moving average of the episodic rewards. For example, the average episodic reward achieved over 10 episodes.\n",
    "- Check if you can make the agent learn faster by using an adaptive learning rate. You start with a high learning rate and you decrease gradually as the agent learns.\n",
    "- Can you change the reward function in order to make the agent learn faster. For example by changing the reward values or by adding new rewards that make the reward function less sparse.\n",
    "\n",
    "- Modify you Q-learing implementation to a SARSA implementation.\n",
    "\n",
    "- Compare Q-learing with SARSA in terms of speed of learning, but especially in terms of the learned policy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving the cartpole environment\n",
    "\n",
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
